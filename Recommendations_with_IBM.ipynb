{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM\n",
    "\n",
    "This notebook will build a recommendation engine to be used on real data from the IBM Watson Studio platform. Recommendations will be made based on different approaches.\n",
    "\n",
    "- Ranked based recommendations (articles with most user interactions)\n",
    "- Collaborative filtering recommendations (finding similar users and suggesting articles not seen by one)\n",
    "- Content based recommendations (classifying articles of interest and recommending articles in that category which have not been seen by the user)\n",
    "- Finally we will look at predictive recommendations using Singular Value Decomposition (SVD)\n",
    "\n",
    "The data for this project was provided by [Udacity Data Science Nanodgree](https://www.udacity.com/course/data-scientist-nanodegree--nd025) in partnership with IBM Watson Studio\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "I. [Exploratory Data Analysis](#Exploratory-Data-Analysis)<br>\n",
    "II. [Rank Based Recommendations](#Rank)<br>\n",
    "III. [User-User Based Collaborative Filtering](#User-User)<br>\n",
    "IV. [Content Based Recommendations](#Content-Recs)<br>\n",
    "V. [Matrix Factorization](#Matrix-Fact)<br>\n",
    "VI. [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/benfarrell/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/benfarrell/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/benfarrell/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title  \\\n",
       "0     1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1     1314.0       healthcare python streaming application demo   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import project_tests as t\n",
    "import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'wordnet', 'averaged_perceptron_tagger'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "df = pd.read_csv('user-item-interactions.csv')\n",
    "df.article_id = df['article_id'].astype(str)\n",
    "df_content = pd.read_csv('articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# Show df to get an idea of the data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.article_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Exploratory-Data-Analysis\">Part I : Exploratory Data Analysis</a>\n",
    "\n",
    "Use the dictionary and cells below to provide some insight into the descriptive statistics of the data.\n",
    "\n",
    "`1.` What is the distribution of how many articles a user interacts with in the dataset?  Provide a visual and descriptive statistics to assist with giving a look at the number of times each user interacts with an article.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=[10,6])\n",
    "df.groupby(['email'])['title'].count().plot(kind='hist',bins=np.arange(0,150))\n",
    "ax.set_xlim(0,40)\n",
    "ax.set_xlabel('Article Iteractions per User')\n",
    "ax.set_title('Histogram for article iteraction per user');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the median and maximum number of user_article interactios below\n",
    "\n",
    "median_val = np.median(df.groupby(['email'])['title'].count().values) # 50% of individuals interact with ____ number of articles or fewer.\n",
    "max_views_by_user = np.max(df.groupby(['email'])['title'].count().values)# The maximum number of user-article interactions by any 1 user is ______."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Explore and remove duplicate articles from the **df_content** dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and explore duplicate articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame: 1056\n",
      "Number of unique article ids: 1051\n",
      "Number of Unique Full Names: 1051\n",
      "Number of Unique Doc Descriptions: 1022\n",
      "Number of Unique Doc Bodys: 1036\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows in DataFrame: {df_content.shape[0]}')\n",
    "print(f'Number of unique article ids: {df_content.article_id.nunique()}')\n",
    "print(f'Number of Unique Full Names: {df_content.doc_full_name.nunique()}')\n",
    "print(f'Number of Unique Doc Descriptions: {df_content.doc_description.nunique()}')\n",
    "print(f'Number of Unique Doc Bodys: {df_content.doc_body.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would suggest that there are a number of duplicates. Lets review those entries with the same descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id\n",
       "50     2\n",
       "221    2\n",
       "232    2\n",
       "398    2\n",
       "577    2\n",
       "Name: doc_full_name, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 5 article numbers which appear to be duplicated\n",
    "df_content.groupby('article_id')['doc_full_name'].count()[df_content.groupby('article_id')['doc_full_name'].count() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>Live</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "232  Homepage Follow Sign in Get started Homepage *...   \n",
       "971  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "232  If you are like most data scientists, you are ...   \n",
       "971  If you are like most data scientists, you are ...   \n",
       "\n",
       "                                         doc_full_name doc_status  article_id  \n",
       "232  Self-service data preparation with IBM Data Re...       Live         232  \n",
       "971  Self-service data preparation with IBM Data Re...       Live         232  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content['article_id'] == 232]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' machine learning is a powerful tool that can easily be merged\\r\\ninto ongoing efforts. Using modularity as an optimization goal provides a\\r\\nprincipled approach to community detection. Local modularity increment can be\\r\\ntweaked to your own dataset to reflect interpretable quantities. This is useful\\r\\nin many scenarios, making it a prime candidate for your everyday toolbox.Many important problems can be represented and studied using graphs — social\\r\\nnetworks, interacting bacterias, brain network modules, hierarchical image\\r\\nclustering and many more.\\r\\n\\r\\nIf we accept graphs as a basic means of structuring and analyzing data about the\\r\\nworld, we shouldn’t be surprised to see them being widely used in Machine\\r\\nLearning as a powerful tool that can enable intuitive properties and power a lot\\r\\nof useful features. '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content['article_id'] == 50].iloc[0]['doc_body'].split('Graph-based')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = df_content[df_content['article_id'] == 232].iloc[0]['doc_body'].split('SELF-SERVICE DATA')[1].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = df_content[df_content['article_id'] == 232].iloc[1]['doc_body'].split('SELF-SERVICE DATA')[1].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring these entries, although the article_id is the same, the content in the body is different for some (article id 50), others (article id 232) it is very similar with only a few words different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_full_name</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_description</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Here’s this week’s news in Data Science and Big Data.</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Here’s this week’s news in Data Science and Big Data.</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Interesting data science links from around the web.</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>How to build SQL Queries in a Scala notebook using IBM Analytics for Apache Spark</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Today’s world of data science leverages data from various sources. Commonly, these sources are Hadoop File System, Enterprise Data Warehouse, Relational Database systems, Enterprise file systems, etc…</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In his latest Write Stuff article Robert Wysocki explains PostgreSQL backups, how to pick which ones to do and what the pros and cons are.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In my last blog, we looked at moving data from Amazon DynamoDB to Cloudant or CouchDB. In this article we’re going to look at extracting data from Microsoft Azure’s DocumentDB service. The tool makes…</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In my previous life I was a particle physicist, searching for very rare nuclear reactions (in order to count neutrinos from the sun and search for dark matter). I had forgotten, until very recently…</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>In order to demystify some of the magic behind machine learning algorithms, I decided to implement a simple machine learning algorithm from scratch.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>“What do you do for a living?” That question used to have a pretty clear answer: “I’m a data scientist.” But lately, it’s gotten more complicated…</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1022 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    doc_full_name\n",
       "doc_description                                                  \n",
       "Here’s this week’s news in Data Science and Big...             16\n",
       "Here’s this week’s news in Data Science and Big...             10\n",
       "Interesting data science links from around the ...              2\n",
       "How to build SQL Queries in a Scala notebook us...              2\n",
       "Today’s world of data science leverages data fr...              2\n",
       "...                                                           ...\n",
       "In his latest Write Stuff article Robert Wysock...              1\n",
       "In my last blog, we looked at moving data from ...              1\n",
       "In my previous life I was a particle physicist,...              1\n",
       "In order to demystify some of the magic behind ...              1\n",
       "“What do you do for a living?” That question us...              1\n",
       "\n",
       "[1022 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Many duplicated doc_descriptions, but many of these are monthly reports/magazines/blogs etc\n",
    "df_content.groupby('doc_description')[['doc_full_name']].count().sort_values(by='doc_full_name',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (May 16, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (March 28, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (February 14, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (January 31, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (August 30, 2016)</td>\n",
       "      <td>Live</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 4, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (March 7, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>839</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (October 05, 2016)</td>\n",
       "      <td>Live</td>\n",
       "      <td>836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (February 21, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "2    ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "78   ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "191  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "210  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "288  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "304  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "560  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "727  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "839  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "867  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "2    Here’s this week’s news in Data Science and Bi...   \n",
       "78   Here’s this week’s news in Data Science and Bi...   \n",
       "191  Here’s this week’s news in Data Science and Bi...   \n",
       "210  Here’s this week’s news in Data Science and Bi...   \n",
       "288  Here’s this week’s news in Data Science and Bi...   \n",
       "304  Here’s this week’s news in Data Science and Bi...   \n",
       "560  Here’s this week’s news in Data Science and Bi...   \n",
       "727  Here’s this week’s news in Data Science and Bi...   \n",
       "839  Here’s this week’s news in Data Science and Bi...   \n",
       "867  Here’s this week’s news in Data Science and Bi...   \n",
       "\n",
       "                                     doc_full_name doc_status  article_id  \n",
       "2       This Week in Data Science (April 18, 2017)       Live           2  \n",
       "78        This Week in Data Science (May 16, 2017)       Live          78  \n",
       "191     This Week in Data Science (March 28, 2017)       Live         191  \n",
       "210  This Week in Data Science (February 14, 2017)       Live         210  \n",
       "288   This Week in Data Science (January 31, 2017)       Live         288  \n",
       "304    This Week in Data Science (August 30, 2016)       Live         304  \n",
       "560      This Week in Data Science (April 4, 2017)       Live         559  \n",
       "727      This Week in Data Science (March 7, 2017)       Live         725  \n",
       "839   This Week in Data Science (October 05, 2016)       Live         836  \n",
       "867  This Week in Data Science (February 21, 2017)       Live         864  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content['doc_description'] == 'Here’s this week’s news in Data Science and Big Data.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to remove ariticle ids which are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any rows that have the same article_id - only keep the first\n",
    "df_content.drop_duplicates(subset='article_id',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use the cells below to find:\n",
    "\n",
    "**a.** The number of unique articles that have an interaction with a user.  \n",
    "**b.** The number of unique articles in the dataset (whether they have any interactions or not).<br>\n",
    "**c.** The number of unique users in the dataset. (excluding null values) <br>\n",
    "**d.** The number of user-article interactions in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_articles = df.article_id.nunique() # The number of unique articles that have at least one interaction\n",
    "total_articles = df_content.shape[0] # The number of unique articles on the IBM platform\n",
    "unique_users = df.email.nunique() # The number of unique users\n",
    "user_article_interactions = df.shape[0] # The number of user-article interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use the cells below to find the most viewed **article_id**, as well as how often it was viewed.  After talking to the company leaders, the `email_mapper` function was deemed a reasonable way to map users to ids.  There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_viewed_article_id = str(df.groupby('article_id')['email'].count().sort_values(ascending=False).index[0]) # The most viewed article in the dataset as a string with one value following the decimal \n",
    "max_views = df.groupby('article_id')['email'].count().sort_values(ascending=False).values[0] # The most viewed article in the dataset was viewed how many times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  article_id                                              title  user_id\n",
       "0     1430.0  using pixiedust for fast, flexible, and easier...        1\n",
       "1     1314.0       healthcare python streaming application demo        2\n",
       "2     1429.0         use deep learning for image classification        3\n",
       "3     1338.0          ml optimization using cognitive assistant        4\n",
       "4     1276.0          deploy your python model as a restful api        5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## map the user email to a user_id column and remove the email column\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# show header\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inbuilt check for Udacity Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you have everything right here! Nice job!\n"
     ]
    }
   ],
   "source": [
    "## If you stored all your results in the variable names above, \n",
    "## you shouldn't need to change anything in this cell\n",
    "\n",
    "sol_1_dict = {\n",
    "    '`50% of individuals have _____ or fewer interactions.`': median_val,\n",
    "    '`The total number of user-article interactions in the dataset is ______.`': user_article_interactions,\n",
    "    '`The maximum number of user-article interactions by any 1 user is ______.`': max_views_by_user,\n",
    "    '`The most viewed article in the dataset was viewed _____ times.`': max_views,\n",
    "    '`The article_id of the most viewed article is ______.`': most_viewed_article_id,\n",
    "    '`The number of unique articles that have at least 1 rating ______.`': unique_articles,\n",
    "    '`The number of unique users in the dataset is ______`': unique_users,\n",
    "    '`The number of unique articles on the IBM platform`': total_articles\n",
    "}\n",
    "\n",
    "# Test your dictionary against the solution\n",
    "t.sol_1_test(sol_1_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Rank\">Part II: Rank-Based Recommendations</a>\n",
    "\n",
    "The data doesn't actually have ratings for whether a user liked an article or not.  We only know that a user has interacted with an article.  In these cases, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "The function below will return the **n** top articles ordered with most interactions as the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = list(df.groupby('title')['user_id'].count().sort_values(ascending=False).index[:n])\n",
    "    \n",
    "    return top_articles # Return the top article titles from df (not df_content)\n",
    "\n",
    "def get_top_article_ids(n, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    n - (int) the number of top articles to return\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    \n",
    "    OUTPUT:\n",
    "    top_articles - (list) A list of the top 'n' article titles \n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    top_articles = list(df.groupby('article_id')['user_id'].count().sort_values(ascending=False).index[:n])\n",
    " \n",
    "    return top_articles # Return the top article ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['use deep learning for image classification', 'insights from new york car accident reports', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'predicting churn with the spss random tree algorithm', 'healthcare python streaming application demo', 'finding optimal locations of new store using decision optimization', 'apache spark lab, part 1: basic concepts', 'analyze energy consumption in buildings', 'gosales transactions for logistic regression model']\n",
      "['1429.0', '1330.0', '1431.0', '1427.0', '1364.0', '1314.0', '1293.0', '1170.0', '1162.0', '1304.0']\n"
     ]
    }
   ],
   "source": [
    "print(get_top_articles(10))\n",
    "print(get_top_article_ids(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inbuilt test for Udacity Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your top_5 looks like the solution list! Nice job.\n",
      "Your top_10 looks like the solution list! Nice job.\n",
      "Your top_20 looks like the solution list! Nice job.\n"
     ]
    }
   ],
   "source": [
    "# Test your function by returning the top 5, 10, and 20 articles\n",
    "top_5 = get_top_articles(5)\n",
    "top_10 = get_top_articles(10)\n",
    "top_20 = get_top_articles(20)\n",
    "\n",
    "# Test each of your three lists from above\n",
    "t.sol_2_test(get_top_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"User-User\">Part III: User-User Based Collaborative Filtering</a>\n",
    "\n",
    "`1.` To build our collaborative filtering recommendation engine, we will first have to be able to classify whether users are similar. To to this, we will create a user article matrix. \n",
    "\n",
    "* Each user will appear just once\n",
    "* Each article will appear just once\n",
    "\n",
    "This matrix will then enable us to check the similarity between users by comparing the dot product of the user article matrix with itself transpossed. \n",
    "\n",
    "users who have interacted (read) the highest number of similar articles will be deemed most similar for this analysis. \n",
    "\n",
    "Given a user_id, we can then search for the closest neighbor (most similar user) and check to see if there are any articles which they have read, which our given user_id has not read. This will allow us to recommend new articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the user-article matrix with 1's and 0's\n",
    "\n",
    "def create_user_item_matrix(df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - pandas dataframe with article_id, title, user_id columns\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item - user item matrix \n",
    "    \n",
    "    Description:\n",
    "    Return a matrix with user ids as rows and article ids on the columns with 1 values where a user interacted with \n",
    "    an article and a 0 otherwise\n",
    "    '''\n",
    "    # Fill in the function here\n",
    "    df_test = df.groupby(['user_id','article_id'])[['title']].count()\n",
    "    df_test['title'] = 1\n",
    "    user_item_df = df_test.unstack()\n",
    "    user_item_df.replace(to_replace = np.nan, value=0, inplace=True)\n",
    "    user_item_df.columns = user_item_df.columns.droplevel()\n",
    "    \n",
    "    return user_item_df # return the user_item matrix \n",
    "\n",
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have passed our quick tests!  Please proceed!\n"
     ]
    }
   ],
   "source": [
    "## Tests: You should just need to run this cell.  Don't change the code.\n",
    "assert user_item.shape[0] == 5149, \"Oops!  The number of users in the user-article matrix doesn't look right.\"\n",
    "assert user_item.shape[1] == 714, \"Oops!  The number of articles in the user-article matrix doesn't look right.\"\n",
    "assert user_item.sum(axis=1)[1] == 36, \"Oops!  The number of articles seen by user 1 doesn't look right.\"\n",
    "print(\"You have passed our quick tests!  Please proceed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` The function below takes a user_id and provides an ordered list of the most similar users to that user (from most similar to least similar).  The returned result should not contain the provided user_id, as we know that each user is similar to him/herself. Because the results for each user here are binary, it (perhaps) makes sense to compute similarity as the dot product of two users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # Make user array\n",
    "    users_array = np.array(user_item.index)\n",
    "    \n",
    "    # compute similarity of each user to the provided user\n",
    "    sims = np.dot(user_item,np.transpose(user_item))\n",
    "    \n",
    "    user_sim_dict = {}\n",
    "    for us, sim in zip(users_array, sims[user_id-1]):\n",
    "        user_sim_dict[us] = sim\n",
    "\n",
    "    # sort by similarity\n",
    "    most_sim_users = [item[0] for item in sorted(user_sim_dict.items(), key = lambda x: x[1], reverse=True)]\n",
    "    # create list of just the ids\n",
    "   \n",
    "    # remove the own user's id\n",
    "    most_sim_users.remove(user_id)\n",
    "    \n",
    "    return  most_sim_users# return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most similar users to user 1 are: [3933, 23, 3782, 203, 4459, 131, 3870, 46, 4201, 49]\n",
      "The 5 most similar users to user 3933 are: [1, 23, 3782, 203, 4459]\n",
      "The 3 most similar users to user 46 are: [4201, 23, 3782]\n"
     ]
    }
   ],
   "source": [
    "# Do a spot check of your function\n",
    "print(\"The 10 most similar users to user 1 are: {}\".format(find_similar_users(1)[:10]))\n",
    "print(\"The 5 most similar users to user 3933 are: {}\".format(find_similar_users(3933)[:5]))\n",
    "print(\"The 3 most similar users to user 46 are: {}\".format(find_similar_users(46)[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` The functions below return the articles you would recommend to a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_ids - (list) a list of article ids\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the title column)\n",
    "    '''\n",
    "    # Your code here\n",
    "    article_names = list(set(df[df.article_id.isin(article_ids)]['title'].values))\n",
    "    \n",
    "    return article_names # Return the article names associated with list of article ids\n",
    "\n",
    "\n",
    "def get_user_articles(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    article_ids - (list) a list of the article ids seen by the user\n",
    "    article_names - (list) a list of article names associated with the list of article ids \n",
    "                    (this is identified by the doc_full_name column in df_content)\n",
    "    \n",
    "    Description:\n",
    "    Provides a list of the article_ids and article titles that have been seen by a user\n",
    "    '''\n",
    "    # Your code here\n",
    "    #Get columns of user_item_mat\n",
    "    cols = np.array(user_item.columns)\n",
    "    \n",
    "    # filter row of matrix, boolean where == 1, convert to np array, find idx of where TRUE, \n",
    "    article_ids_float = list(cols[np.where(np.array(user_item.loc[user_id] == 1) == True)[0]])\n",
    "    article_ids_str = [str(i) for i in article_ids_float]\n",
    "       \n",
    "    \n",
    "    article_names = list(set(df[df['article_id'].isin(article_ids_str)]['title'].values))\n",
    "    \n",
    "    return article_ids_str, article_names # return the ids and names\n",
    "\n",
    "\n",
    "def user_user_recs(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    Users who are the same closeness are chosen arbitrarily as the 'next' user\n",
    "    \n",
    "    For the user where the number of recommended articles starts below m \n",
    "    and ends exceeding m, the last items are chosen arbitrarily\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    \n",
    "    # Find articles already viewed\n",
    "    viewed_ids = get_user_articles(user_id, user_item=user_item)[0]\n",
    "    \n",
    "    #Find similar users\n",
    "    similar_users = find_similar_users(user_id)\n",
    "    \n",
    "    recs = []\n",
    "    for user in similar_users:\n",
    "        #Check if recs met\n",
    "        if len(recs) < m:\n",
    "            # Find out what they have seen\n",
    "            for article in get_user_articles(user)[0]:\n",
    "                if len(recs) == m:\n",
    "                    break\n",
    "                else:\n",
    "                    if article not in viewed_ids or recs:\n",
    "                        recs.append(article)\n",
    "    \n",
    "    return recs # return your recommendations for this user_id    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['1320.0', '232.0', '844.0'],\n",
       " ['self-service data preparation with ibm data refinery',\n",
       "  'use the cloudant-spark connector in python notebook',\n",
       "  'housing (2015): united states demographic measures'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_user_articles(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a tensorflow regression model to predict house values',\n",
       " 'recommender systems: approaches & algorithms',\n",
       " 'data tidying in data science experience',\n",
       " 'airbnb data for analytics: vancouver listings',\n",
       " 'access db2 warehouse on cloud and db2 with python',\n",
       " '1448    i ranked every intro to data science course on...\\nName: title, dtype: object',\n",
       " 'airbnb data for analytics: mallorca reviews',\n",
       " '520    using notebooks with pixiedust for fast, flexi...\\nName: title, dtype: object',\n",
       " 'tensorflow quick tips',\n",
       " 'analyze facebook data using ibm watson and watson studio']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check Results\n",
    "get_article_names(user_user_recs(1, 10)) # Return 10 recommendations for user 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If this is all you see, you passed all of our tests!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "# Test your functions here - No need to change this code - just run this cell\n",
    "assert set(get_article_names(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_article_names(['1320.0', '232.0', '844.0'])) == set(['housing (2015): united states demographic measures','self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook']), \"Oops! Your the get_article_names function doesn't work quite how we expect.\"\n",
    "assert set(get_user_articles(20)[0]) == set(['1320.0', '232.0', '844.0']),\"Oops doesn\\'t look right\"\n",
    "assert set(get_user_articles(20)[1]) == set(['housing (2015): united states demographic measures', 'self-service data preparation with ibm data refinery','use the cloudant-spark connector in python notebook'])\n",
    "assert set(get_user_articles(2)[0]) == set(['1024.0', '1176.0', '1305.0', '1314.0', '1422.0', '1427.0'])\n",
    "assert set(get_user_articles(2)[1]) == set(['using deep learning to reconstruct high-resolution audio', 'build a python app on the streaming analytics service', 'gosales transactions for naive bayes model', 'healthcare python streaming application demo', 'use r dataframes & ibm watson natural language understanding', 'use xgboost, scikit-learn & ibm watson machine learning apis'])\n",
    "print(\"If this is all you see, you passed all of our tests!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Now we are going to improve the consistency of the **user_user_recs** function from above.  \n",
    "\n",
    "* Instead of arbitrarily choosing when we obtain users who are all the same closeness to a given user - choose the users that have the most total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "\n",
    "* Instead of arbitrarily choosing articles from the user where the number of recommended articles starts below m and ends exceeding m, choose articles with the articles with the most total interactions before choosing those with fewer total interactions. This ranking should be  what would be obtained from the **top_articles** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_users2(user_id, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user_id\n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "                1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "    OUTPUT:\n",
    "    similar_users - (list) an ordered list where the closest users (largest dot product users)\n",
    "                    are listed first\n",
    "    \n",
    "    Description:\n",
    "    Computes the similarity of every pair of users based on the dot product\n",
    "    Returns an ordered\n",
    "    \n",
    "    '''\n",
    "    # Make user array\n",
    "    users_array = np.array(user_item.index)\n",
    "    \n",
    "    # compute similarity of each user to the provided user\n",
    "    sims = np.dot(user_item,np.transpose(user_item))\n",
    "    \n",
    "    user_sim_dict = {}\n",
    "    for us, sim in zip(users_array, sims[user_id-1]):\n",
    "        user_sim_dict[us] = sim\n",
    "\n",
    "    # sort by similarity\n",
    "    most_sim_users = sorted(user_sim_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create pandas dataframe of user_ids, similarity score, avoiding 1st value which is user_id\n",
    "    # All sort by similarity and then by num_interactions\n",
    "    new_df = {'neighbor_id':[user[0] for user in most_sim_users[1:]],\n",
    "              'similarity':[user[1] for user in most_sim_users[1:]],\n",
    "              'num_interactions':[user_item.loc[user[0]].sum() for user in most_sim_users[1:]]\n",
    "             }.sort_values(by=['similarity','num_interactions'],ascending=False)\n",
    "    \n",
    "    # Add column for number of interactions\n",
    "    \n",
    "    new_df = pd.DataFrame(new_df)\n",
    "    return  new_df# return a list of the users in order from most to least similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook \n",
    "    user_item - (pandas dataframe) matrix of users by articles: \n",
    "            1's when a user has interacted with an article, 0 otherwise\n",
    "    \n",
    "            \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    neighbor_id - is a neighbor user_id\n",
    "                    similarity - measure of the similarity of each user to the provided user_id\n",
    "                    num_interactions - the number of articles viewed by the user - if a u\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    # Your code here\n",
    "        # Make user array\n",
    "    users_array = np.array(user_item.index)\n",
    "    \n",
    "    # compute similarity of each user to the provided user\n",
    "    sims = np.dot(user_item,np.transpose(user_item))\n",
    "    \n",
    "    user_sim_dict = {}\n",
    "    for us, sim in zip(users_array, sims[user_id-1]):\n",
    "        user_sim_dict[us] = sim\n",
    "\n",
    "    # sort by similarity\n",
    "    most_sim_users = sorted(user_sim_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create pandas dataframe of user_ids, similarity score, avoiding 1st value which is user_id\n",
    "    # All sort by similarity and then by num_interactions\n",
    "    new_df = {'neighbor_id':[user[0] for user in most_sim_users[1:]],\n",
    "              'similarity':[user[1] for user in most_sim_users[1:]],\n",
    "              'num_interactions':[user_item.loc[user[0]].sum() for user in most_sim_users[1:]]\n",
    "             }\n",
    "    \n",
    "    # Add column for number of interactions\n",
    "    \n",
    "    neighbors_df = pd.DataFrame(new_df)\n",
    "    neighbors_df = neighbors_df.sort_values(by=['similarity','num_interactions'],ascending=False)\n",
    "    \n",
    "    return neighbors_df # Return the dataframe specified in the doc_string\n",
    "\n",
    "\n",
    "def user_user_recs_part2(user_id, m=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    user_id - (int) a user id\n",
    "    m - (int) the number of recommendations you want for the user\n",
    "    \n",
    "    OUTPUT:\n",
    "    recs - (list) a list of recommendations for the user by article id\n",
    "    rec_names - (list) a list of recommendations for the user by article title\n",
    "    \n",
    "    Description:\n",
    "    Loops through the users based on closeness to the input user_id\n",
    "    For each user - finds articles the user hasn't seen before and provides them as recs\n",
    "    Does this until m recommendations are found\n",
    "    \n",
    "    Notes:\n",
    "    * Choose the users that have the most total article interactions \n",
    "    before choosing those with fewer article interactions.\n",
    "\n",
    "    * Choose articles with the articles with the most total interactions \n",
    "    before choosing those with fewer total interactions. \n",
    "   \n",
    "    '''\n",
    "    # Your code here\n",
    "    # Find articles already viewed by user\n",
    "    viewed_ids = get_user_articles(user_id, user_item=user_item)[0]\n",
    "    \n",
    "    recs = []\n",
    "    \n",
    "    for user in get_top_sorted_users(user_id)['neighbor_id']:\n",
    "        if recs == m:\n",
    "            break\n",
    "        else:\n",
    "            article_ids = df[df['user_id'] == 1].groupby('article_id')[['title']].count().sort_values(by='title',ascending=False).index\n",
    "            for art_id in article_ids:\n",
    "                if len(recs) == m:\n",
    "                    break\n",
    "                else:\n",
    "                    if art_id not in viewed_ids or recs:\n",
    "                        recs.append(art_id)\n",
    "    \n",
    "    rec_names = [df[df['article_id'] == ids]['title'].values[0] for ids in recs]\n",
    "    \n",
    "    return recs, rec_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "['1052.0', '1363.0', '1431.0', '1430.0', '585.0', '1406.0', '668.0', '310.0', '1185.0', '1183.0']\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['access db2 warehouse on cloud and db2 with python', 'predict loan applicant behavior with tensorflow neural networking', 'visualize car data with brunel', 'using pixiedust for fast, flexible, and easier data analysis and experimentation', 'tidyverse practice: mapping large european cities', 'uci: iris', 'shiny: a data scientist’s best friend', 'time series prediction using recurrent neural networks (lstms)', 'classify tumors with machine learning', 'categorize urban density']\n"
     ]
    }
   ],
   "source": [
    "# Quick spot check for Udacity Submission\n",
    "rec_ids, rec_names = user_user_recs_part2(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Use your functions from above to correctly fill in the solutions to the dictionary below.  Then test your dictionary against the solution.  Provide the code you need to answer each following the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tests with a dictionary of results\n",
    "\n",
    "user1_most_sim = find_similar_users(1)[0] # Find the user that is most similar to user 1 \n",
    "user131_10th_sim = find_similar_users(131)[9]# Find the 10th most similar user to user 131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This all looks good!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "## Dictionary Test Here\n",
    "sol_5_dict = {\n",
    "    'The user that is most similar to user 1.': user1_most_sim, \n",
    "    'The user that is the 10th most similar to user 131': user131_10th_sim,\n",
    "}\n",
    "\n",
    "t.sol_5_test(sol_5_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`6.` If we were given a new user, which of the above functions would you be able to use to make recommendations?  Explain.  Can you think of a better way we might make recommendations?  Use the cell below to explain a better method for new users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Provide your response here.**\n",
    "\n",
    "Because the user is new, we won't be able to recommend articles based on collaborative filtering because they will not be similar to any other user. The best option is to simply recommend the top rated (most viewed) articles. These have the highest probability of being well received by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`7.` Using your existing functions, provide the top 10 recommended articles you would provide for the a new user below.  You can test your function against our thoughts to make sure we are all on the same page with how we might make a recommendation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_user = '0.0'\n",
    "\n",
    "# What would your recommendations be for this new user '0.0'?  As a new user, they have no observed articles.\n",
    "# Provide a list of the top 10 article ids you would give to \n",
    "new_user_recs = get_top_article_ids(10)# Your recommendations here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's right!  Nice job!\n"
     ]
    }
   ],
   "source": [
    "assert set(new_user_recs) == set(['1314.0','1429.0','1293.0','1427.0','1162.0','1364.0','1304.0','1170.0','1431.0','1330.0']), \"Oops!  It makes sense that in this case we would want to recommend the most popular articles, because we don't know anything about these users.\"\n",
    "\n",
    "print(\"That's right!  Nice job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Content-Recs\">Part IV: Content Based Recommendations </a>\n",
    "\n",
    "**The bulk of the work for the LDA model was developed in a separate notebook** `LDA_IBM_Classification.ipynb`. For details on how the model was optimised for number of topics and removal of unnecessary stop words and more please see notebook.\n",
    "\n",
    "Another method used to make recommendations is to perform a ranking of the highest ranked articles associated with some term. This is a content based recommendation.\n",
    "\n",
    "In the below recommendation engine, we will apply some natural language processing to the body of the documents in an antempt to classify them. Once classified, it will enable us to determine similarity of other documents and recommend them.\n",
    "\n",
    "`1.Text Preprocessing`\n",
    "The first step in this process is to pre-process the text in our documents. This quality of the preprocessing is an vital part of the analyis to ensure meaningful results are obtained from the NLP.\n",
    "\n",
    "Steps \n",
    "- Define Stop Words\n",
    "- Tokenize\n",
    "- Remove stop words\n",
    "- Create bigrams\n",
    "- Lemmatization\n",
    "- Create Dictionary and Corpus\n",
    "\n",
    "`2. Build Model`\n",
    "Use LDA Model to classify docs into Topics\n",
    "\n",
    "`3. Evaluate Model`\n",
    "Then visualize the topics using pyLDAvis and check Coherance and perplexity scores. \n",
    "\n",
    "Iteratively improve the model by adding more stop words, changing threshold for bigrams, finding optimum number of topics, sanity checkin the topics that are created.\n",
    "\n",
    "`4. Utilise Final Model to Classify Articles`\n",
    "Utilise final model to classify the article on topic percentage contributions. This will allow an article topic matrix to be generated. Taking the RMS of the dot product of this matrix with its transpose will show the similarity of articles. Those with smallest difference will be most similar.\n",
    "\n",
    "When given an article, it is then possible to provide a list of similar articles. \n",
    "\n",
    "When given an article and a user, you can then suggest similar articles which the user hasn't seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# This list of stop words was built up incrementally\n",
    "additional_stops = (['add',\n",
    "                     'close',\n",
    "                     'click',\n",
    "                     'datum',\n",
    "                     'document',\n",
    "                     'duration',\n",
    "                     'email',\n",
    "                     'go',\n",
    "                     'get',\n",
    "                     'guide',\n",
    "                     'index',\n",
    "                     'http',\n",
    "                     'less',\n",
    "                     'like',\n",
    "                     'load',\n",
    "                     'loading',\n",
    "                     'login',\n",
    "                     'name',\n",
    "                     'make',\n",
    "                     'more',\n",
    "                     'new',\n",
    "                     'one',\n",
    "                     'pass',\n",
    "                     'point',\n",
    "                     'queue',\n",
    "                     'show',\n",
    "                     'self',\n",
    "                     'service',\n",
    "                     'sign',\n",
    "                     'signout',\n",
    "                     'start',\n",
    "                     'starting',\n",
    "                     'stop',\n",
    "                     'subscribe',\n",
    "                     'undo',\n",
    "                     'use',\n",
    "                     'user',\n",
    "                     'using',\n",
    "                     'video',\n",
    "                     'view',\n",
    "                     'watch',\n",
    "                     'work'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = list(df_content.doc_body.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=75) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words and len(word)>2] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def lemmatization(texts):\n",
    "    #instantiate word lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    return [[lemmatizer.lemmatize(word) for word in doc] for doc in texts]\n",
    "\n",
    "# Remove stop words\n",
    "data_without_stops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_with_bigrams = make_bigrams(data_without_stops)\n",
    "\n",
    "#instantiate word lemmatizer\n",
    "data_lemmatized = lemmatization(data_with_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import topics determined by LDA Analysis\n",
    "topics_df = pd.read_csv('Topics_df.csv')\n",
    "del topics_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TopicId</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Training_Machine_Learning_Algorithm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Data_Analytics_IBM_Spark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Cloudant_Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>rstudio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Graph_Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Redis_Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>DataScience_MachineLearning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>EDA_Aggregating_Data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>IBM_Watson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>MongoDB_Database</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Python_Pandas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Undefined</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    TopicId                                 Name\n",
       "0         0  Training_Machine_Learning_Algorithm\n",
       "1         1             Data_Analytics_IBM_Spark\n",
       "2         2                    Cloudant_Database\n",
       "3         3                              rstudio\n",
       "4         4                         Graph_Theory\n",
       "5         5                       Redis_Database\n",
       "6         6          DataScience_MachineLearning\n",
       "7         7                 EDA_Aggregating_Data\n",
       "8         8                           IBM_Watson\n",
       "9         9                     MongoDB_Database\n",
       "10       10                        Python_Pandas\n",
       "11       11                            Undefined"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Model - Importing Model which was optimised in LDA_IBM_Classification.ipynb\n",
    "lda_model = joblib.load('LDA_Topic_Classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_assignment(text, model=lda_model, topics=topics_df['Name']):\n",
    "    '''\n",
    "    args: (str) text: text to be classified\n",
    "          (model) model: lda model\n",
    "          (list) topics: topics classified by lda\n",
    "    \n",
    "    returns: (list)(tuples): Descending List of tuples: [((int) Most relevant topic based on LDA model, (float) Perc Contribution)]\n",
    "       \n",
    "    '''\n",
    "    #Preprocess text\n",
    "    text = [text]\n",
    "    data = list(sent_to_words(text))\n",
    "    data = remove_stopwords(data)\n",
    "    data = make_bigrams(data)\n",
    "    data = lemmatization(data)\n",
    "    data = id2word.doc2bow(data[0])\n",
    "    \n",
    "    #Classify Text\n",
    "    top_topic = sorted(model[data][0], key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return top_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1044\n",
      "1045\n",
      "1047\n",
      "1048\n"
     ]
    }
   ],
   "source": [
    "# Create list of relevant topics for each document and include in dataframe\n",
    "scoring = []\n",
    "for i, content in enumerate(df_content['doc_body']):\n",
    "    try:\n",
    "        scoring.append(topic_assignment(content))\n",
    "    except:\n",
    "        print(i)\n",
    "        scoring.append([np.nan])\n",
    "\n",
    "df_content['Topic_Rank'] = scoring        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function to extract percentage contribution of each topic\n",
    "def extract_topic_perc(rank, top_id):\n",
    "    try:\n",
    "        idx = [top[0] for top in rank].index(top_id)\n",
    "        return rank[idx][1]\n",
    "    \n",
    "    except:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Additional Columns for 12 Topics and Allocate Percentage Contributions for each Document\n",
    "for i in range(12):\n",
    "    col_title = 'T'+str(i)\n",
    "    df_content[col_title] = df_content['Topic_Rank'].apply(lambda x: 100*extract_topic_perc(x,i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create article topic matrix - This will be required for checking similarity between docs\n",
    "cols = ['article_id']\n",
    "for i in range(12):\n",
    "    cols.append('T'+str(i))\n",
    "    \n",
    "article_topics_mat = df_content[cols].set_index('article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_articles(article_id, df=df, df2=df_content, article_item=article_topics_mat, num=10):\n",
    "    '''\n",
    "    INPUT:\n",
    "    article_id - (int)\n",
    "    df - (pandas dataframe) df as defined at the top of the notebook - article_user interactions \n",
    "    df2 - (pandas dataframe) df_content as defined at the top of the notebook - article info\n",
    "    article_item - (pandas dataframe) matrix of article by topics (12): decimal contribution of each topic\n",
    "    num - Number of article to recommend (int)\n",
    "    \n",
    "    OUTPUT:\n",
    "    neighbors_df - (pandas dataframe) a dataframe with:\n",
    "                    article_id - is an article_id\n",
    "                    article title\n",
    "                    \n",
    "    Other Details - sort the neighbors_df by the similarity and then by number of interactions where \n",
    "                    highest of each is higher in the dataframe\n",
    "     \n",
    "    '''\n",
    "    #Check that article is within corpus evaluated\n",
    "    if article_id > 1050 or article_id < 0:\n",
    "        return 'Not available for Content Based Recommendation'\n",
    "\n",
    "    # Make article array\n",
    "    users_array = np.array(article_item.index)\n",
    "    \n",
    "    # compute similarity of each article to the provided user\n",
    "    sims = np.dot(article_item,np.transpose(article_item))\n",
    "    \n",
    "    user_sim_dict = {}\n",
    "    for us, sim in zip(users_array, sims[article_id]):\n",
    "        # We need to find the rms error \n",
    "        user_sim_dict[us] = np.sqrt(np.square(sim - sims[article_id][article_id]))\n",
    "\n",
    "    # sort by similarity of articles\n",
    "    most_sim_users = sorted(user_sim_dict.items(), key = lambda x: x[1])\n",
    "    \n",
    "    # Create list of title - Not every article has been read by a user\n",
    "    titles, num_interactions = [], []\n",
    "    for user in most_sim_users[1:]:\n",
    "        try:\n",
    "            titles.append(df[df['article_id'] == str(float(user[0]))]['title'].values[0])\n",
    "            num_interactions.append(sum(df['article_id'] == str(float(user[0]))))\n",
    "            \n",
    "        except:\n",
    "            titles.append(df2[df2['article_id'] == user[0]]['doc_full_name'].values[0])\n",
    "            num_interactions.append(0)\n",
    "    \n",
    "    # Create pandas dataframe of article_ids, similarity score, avoiding 1st value which is user_id\n",
    "    # All sort by similarity and then by num_interactions\n",
    "    new_df = {'Article_Id':[user[0] for user in most_sim_users[1:]],\n",
    "              'Title': titles,\n",
    "              'similarity':[user[1] for user in most_sim_users[1:]],\n",
    "              'num_interactions':num_interactions\n",
    "             }\n",
    "    \n",
    "    # Create DataFrame and SortBy: Similarity then User Interactions\n",
    "\n",
    "    neighbors_df = pd.DataFrame(new_df)\n",
    "    neighbors_df = neighbors_df.sort_values(by=['similarity','num_interactions'],ascending=[True,False])\n",
    "    \n",
    "    return neighbors_df[['Article_Id','Title']].head(num) # Return the dataframe specified in the doc_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_articles_for_user(user_id, article_id, df=df, num=10):\n",
    "    \n",
    "    '''\n",
    "    args\n",
    "    user_id (int)\n",
    "    article_id (int)\n",
    "    df - (dataframe) as defined in notebook\n",
    "    num (int) number of articles requested\n",
    "    \n",
    "    return \n",
    "    \n",
    "    dataframe of article_ids and article_titles\n",
    "    \n",
    "    '''\n",
    "    #Check that article is within corpus evaluated\n",
    "    if article_id > 1050 or article_id < 0:\n",
    "        return 'Not available for Content Based Recommendation'\n",
    "    \n",
    "    # Find articles already seen by user\n",
    "    viewed = list(set(df[df['user_id'] == user_id]['article_id'].values))\n",
    "    # Convert from str to int\n",
    "    viewed = [int(float(a_id)) for a_id in viewed]\n",
    "    \n",
    "    \n",
    "    # Get user agnostic recommendations based on content\n",
    "    df_rec = find_similar_articles(article_id, df=df, df2=df_content, article_item=article_topics_mat, num=num)\n",
    "    \n",
    "    return df_rec[~df_rec['Article_Id'].isin(viewed)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of documents\n",
    "\n",
    "This knowledge based recommendation engine used Latent Dirichlet Allocation(LDA), an unsupervised machine learning algorithm, to identify hidden topics within the corpus of documents. This was particularly useful because the articles in this dataset were not labelled. This problem lended itself well to unsupervised ML for classification.\n",
    "\n",
    "The LDA approach utilised Python Gensim package. \n",
    "\n",
    "**The bulk of the work for the LDA model was developed in a separate notebook** `LDA_IBM_Classification.ipynb`. For details on how the model was optimised for number of topics and removal of unnecessary stop words and more please see notebook.\n",
    "\n",
    "The process of developing the topics is outlined below:\n",
    "\n",
    "- Extract document bodies from csv file into a list for processing\n",
    "- Prepare Stopwords - iterative process which developed over model creation.\n",
    "- Normalize Text\n",
    "- Tokenize words\n",
    "- Creating Bigram Models\n",
    "- Remove Stopwords, Make Bigrams and Lemmatize\n",
    "- Create the Dictionary and Corpus needed for Topic Modeling\n",
    "- Building the LDA Model\n",
    "- View the topics in LDA model\n",
    "- Evaluate the Model: Perplexity and Coherence Scores\n",
    "- Visualize the topics-keywords\n",
    "- Optimising the number of topics\n",
    "\n",
    "To expand on a few points from above. After creating the initial model and visualizing the topics, it was evident that there was heavy overlap between the topics which suggested this could be optimised. \n",
    "\n",
    "When looking through the words which made up the percentage contribution of the topics, it was evident that some topics could be removed because they were not well defined. I also generated several models, incrementally increasing the number of topics and measuring the coherance. It was evident that 16 topics provided the highest level of coherance but some topics were difficult to classify, so it was opted to choose 12 topics instead. When visualising the topics, these were then clearly separted and well proportioned.\n",
    "\n",
    "I created a small table to classify the topics which were identified from the LDA analysis. These were:\n",
    "\n",
    "\n",
    "|Topic_id. | Topic |\n",
    "| :---:    | :--- |\n",
    "0 |Training_Machine_Learning_Algorithm\n",
    "1 |  Data_Analytics_IBM_Spark\n",
    "2 |                      Cloudant_Database\n",
    "3 |                                rstudio\n",
    "4 |                           Graph_Theory\n",
    "5 |                         Redis_Database\n",
    "6 |            DataScience_MachineLearning\n",
    "7 |                   EDA_Aggregating_Data\n",
    "8 |                             IBM_Watson\n",
    "9 |                       MongoDB_Database\n",
    "10|                          Python_Pandas\n",
    "11|                         Undefined (<1%)\n",
    "\n",
    "\n",
    "### Building content based recommendation engine\n",
    "\n",
    "Once the documents were classified the recommendation engine was built. This is defined in the function \n",
    "\n",
    "**find_similar_articles(article_id, df=df, df2=df_content, article_item=article_topics_mat, num=10)**\n",
    "\n",
    "The process followed was:\n",
    "\n",
    "Given an article_id and a requested number of recommendations, recommend a list of articles based on similarity to the article_id provided and the popularity of the article in the data set (number of user interactions). Articles with an equivalent similarity woudl be then ordered by number of interactions.\n",
    "\n",
    "The similarity of articles was defined by performing the dot product between the article topic matrix and its transpose. Then the RMS error was found. Those articles with the least error were said to be the most similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_Id</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568</td>\n",
       "      <td>Build an iOS 8 App with Bluemix and the Mobile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536</td>\n",
       "      <td>Embed rich reports in your applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>697</td>\n",
       "      <td>DataLayer Conference: Online Schema Migrations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>ibm-cds-labs/hybrid-cloud-tutorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>285</td>\n",
       "      <td>Metrics Maven: Crosstab Revisited - Pivoting W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>653</td>\n",
       "      <td>create a connection and add it to a project us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>499</td>\n",
       "      <td>let data dictate the visualization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>77</td>\n",
       "      <td>apache spark @scale: a 60 tb+ production use case</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>746</td>\n",
       "      <td>httr 1.2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>786</td>\n",
       "      <td>Getting Connected with RabbitMQ and Elasticsearch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_Id                                              Title\n",
       "0         568  Build an iOS 8 App with Bluemix and the Mobile...\n",
       "1         536            Embed rich reports in your applications\n",
       "2         697  DataLayer Conference: Online Schema Migrations...\n",
       "3         149                 ibm-cds-labs/hybrid-cloud-tutorial\n",
       "4         285  Metrics Maven: Crosstab Revisited - Pivoting W...\n",
       "5         653  create a connection and add it to a project us...\n",
       "6         499                 let data dictate the visualization\n",
       "7          77  apache spark @scale: a 60 tb+ production use case\n",
       "8         746                                         httr 1.2.0\n",
       "9         786  Getting Connected with RabbitMQ and Elasticsearch"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make recommendations for a brand new user\n",
    "# I would have no content to base this off so I would simply provide the most popular articles \n",
    "\n",
    "new_user_recs = get_top_article_ids(10)# Your recommendations here\n",
    "\n",
    "# make a recommendations for a user who only has interacted with article id '980.0'\n",
    "find_similar_articles(980)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>871.0</td>\n",
       "      <td>overfitting in machine learning: what it is an...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>887.0</td>\n",
       "      <td>502    forgetting the past to learn the future...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>871.0</td>\n",
       "      <td>overfitting in machine learning: what it is an...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>871.0</td>\n",
       "      <td>overfitting in machine learning: what it is an...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6922</th>\n",
       "      <td>871.0</td>\n",
       "      <td>overfitting in machine learning: what it is an...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20918</th>\n",
       "      <td>347.0</td>\n",
       "      <td>announcing dsx environments in beta!</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23934</th>\n",
       "      <td>1160.0</td>\n",
       "      <td>analyze accident reports on amazon emr spark</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24853</th>\n",
       "      <td>34.0</td>\n",
       "      <td>top 10 machine learning use cases: part 1</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      article_id                                              title  user_id\n",
       "77         871.0  overfitting in machine learning: what it is an...       50\n",
       "210        887.0  502    forgetting the past to learn the future...       50\n",
       "331        871.0  overfitting in machine learning: what it is an...       50\n",
       "3546       871.0  overfitting in machine learning: what it is an...       50\n",
       "6922       871.0  overfitting in machine learning: what it is an...       50\n",
       "20918      347.0               announcing dsx environments in beta!       50\n",
       "23934     1160.0       analyze accident reports on amazon emr spark       50\n",
       "24853       34.0          top 10 machine learning use cases: part 1       50"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['user_id'] == 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_Id</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>229</td>\n",
       "      <td>Serverless Data Flow Sequencing with Watson Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>134</td>\n",
       "      <td>this week in data science (november 01, 2016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>139</td>\n",
       "      <td>Designing the UFC Moneyball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "      <td>Picking SQL or NoSQL? – A Compose View</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>995</td>\n",
       "      <td>Visualizing weather data as a PixieApp – IBM W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>99</td>\n",
       "      <td>Seven Databases in Seven Days – Day 2: MongoDB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>795</td>\n",
       "      <td>7292    a dramatic tour through python’s data ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>534</td>\n",
       "      <td>dplyr 0.5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>The Conversational Interface is the New Paradigm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>391</td>\n",
       "      <td>Building better database bridges with the new ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_Id                                              Title\n",
       "0         229  Serverless Data Flow Sequencing with Watson Da...\n",
       "1         134      this week in data science (november 01, 2016)\n",
       "2         139                        Designing the UFC Moneyball\n",
       "3         121             Picking SQL or NoSQL? – A Compose View\n",
       "4         995  Visualizing weather data as a PixieApp – IBM W...\n",
       "5          99     Seven Databases in Seven Days – Day 2: MongoDB\n",
       "6         795  7292    a dramatic tour through python’s data ...\n",
       "7         534                                        dplyr 0.5.0\n",
       "8          24   The Conversational Interface is the New Paradigm\n",
       "9         391  Building better database bridges with the new ..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recommend articles for user_id 50 given article_id 34\n",
    "recommend_articles_for_user(50,34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"Matrix-Fact\">Part V: Matrix Factorization</a>\n",
    "\n",
    "In this part of the notebook, we will use matrix factorization to make article recommendations to the users on the IBM Watson Studio platform.\n",
    "\n",
    "`1.` We have already created a **user_item** matrix above in **question 1** of **Part III** above.  T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the matrix here\n",
    "user_item_matrix = pd.read_pickle('user_item_matrix.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>article_id</th>\n",
       "      <th>0.0</th>\n",
       "      <th>100.0</th>\n",
       "      <th>1000.0</th>\n",
       "      <th>1004.0</th>\n",
       "      <th>1006.0</th>\n",
       "      <th>1008.0</th>\n",
       "      <th>101.0</th>\n",
       "      <th>1014.0</th>\n",
       "      <th>1015.0</th>\n",
       "      <th>1016.0</th>\n",
       "      <th>...</th>\n",
       "      <th>977.0</th>\n",
       "      <th>98.0</th>\n",
       "      <th>981.0</th>\n",
       "      <th>984.0</th>\n",
       "      <th>985.0</th>\n",
       "      <th>986.0</th>\n",
       "      <th>990.0</th>\n",
       "      <th>993.0</th>\n",
       "      <th>996.0</th>\n",
       "      <th>997.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 714 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "article_id  0.0  100.0  1000.0  1004.0  1006.0  1008.0  101.0  1014.0  1015.0  \\\n",
       "user_id                                                                         \n",
       "1           0.0    0.0     0.0     0.0     0.0     0.0    0.0     0.0     0.0   \n",
       "2           0.0    0.0     0.0     0.0     0.0     0.0    0.0     0.0     0.0   \n",
       "3           0.0    0.0     0.0     0.0     0.0     0.0    0.0     0.0     0.0   \n",
       "4           0.0    0.0     0.0     0.0     0.0     0.0    0.0     0.0     0.0   \n",
       "5           0.0    0.0     0.0     0.0     0.0     0.0    0.0     0.0     0.0   \n",
       "\n",
       "article_id  1016.0  ...  977.0  98.0  981.0  984.0  985.0  986.0  990.0  \\\n",
       "user_id             ...                                                   \n",
       "1              0.0  ...    0.0   0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "2              0.0  ...    0.0   0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "3              0.0  ...    1.0   0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "4              0.0  ...    0.0   0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "5              0.0  ...    0.0   0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "article_id  993.0  996.0  997.0  \n",
       "user_id                          \n",
       "1             0.0    0.0    0.0  \n",
       "2             0.0    0.0    0.0  \n",
       "3             0.0    0.0    0.0  \n",
       "4             0.0    0.0    0.0  \n",
       "5             0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 714 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick look at the matrix\n",
    "user_item_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` In this situation, you can use Singular Value Decomposition from [numpy](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.linalg.svd.html) on the user-item matrix.  Use the cell to perform SVD, and explain why this is different than in the lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform SVD on the User-Item Matrix Here\n",
    "u, s, vt = np.linalg.svd(user_item_matrix)# use the built in to get the three matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5149, 5149)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714,)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 714)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "In this application we are able to simply use numpy's inbuild SVD becuase our user article matrix has no missing data. In previous examples (outside of this notebook) there was missing data which would prevent the SVD from converging. In that case we resolved this with funkSVD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Now for the tricky part, how do we choose the number of latent features to use?  Running the below cell, you can see that as the number of latent features increases, we obtain a lower error rate on making predictions for the 1 and 0 values in the user-item matrix.  Run the cell below to get an idea of how the accuracy improves as we increase the number of latent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s[:k]), u[:, :k], vt[:k, :]\n",
    "    \n",
    "    # take dot product\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix, user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` From the above, we can't really be sure how many features to use, because simply having a better way to predict the 1's and 0's of the matrix doesn't exactly give us an indication of if we are able to make good recommendations.  Instead, we might split our dataset into a training and test set of data, as shown in the cell below.  \n",
    "\n",
    "Use the code from question 3 to understand the impact on accuracy of the training and test sets of data with different numbers of latent features. Using the split below: \n",
    "\n",
    "* How many users can we make predictions for in the test set?  \n",
    "* How many users are we not able to make predictions for because of the cold start problem?\n",
    "* How many articles can we make predictions for in the test set?  \n",
    "* How many articles are we not able to make predictions for because of the cold start problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5149"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df.head(40000)\n",
    "df_test = df.tail(5993)\n",
    "\n",
    "def create_test_and_train_user_item(df_train, df_test):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df_train - training dataframe\n",
    "    df_test - test dataframe\n",
    "    \n",
    "    OUTPUT:\n",
    "    user_item_train - a user-item matrix of the training dataframe \n",
    "                      (unique users for each row and unique articles for each column)\n",
    "    user_item_test - a user-item matrix of the testing dataframe \n",
    "                    (unique users for each row and unique articles for each column)\n",
    "    test_idx - all of the test user ids\n",
    "    test_arts - all of the test article ids\n",
    "    \n",
    "    '''\n",
    "    # Your code here\n",
    "    user_item_train = create_user_item_matrix(df_train)\n",
    "    user_item_test = create_user_item_matrix(df_test)\n",
    "    \n",
    "    test_idx = df_test.index\n",
    "    test_arts = df_test.columns\n",
    "    \n",
    "    return user_item_train, user_item_test, test_idx, test_arts\n",
    "\n",
    "user_item_train, user_item_test, test_idx, test_arts = create_test_and_train_user_item(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intersect between train and test (These are the users we can predict on)\n",
    "users_oi = np.intersect1d(df_test.user_id.unique(), df_train.user_id.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find index of train set for comparison\n",
    "idx_train = np.where(np.isin(user_item_train.index,users_oi))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find index of Test set for Comparison\n",
    "idx_test = np.where(np.isin(user_item_test.index,users_oi))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users we can make predictions for in the test set: 20\n"
     ]
    }
   ],
   "source": [
    "# Check users in test set which are also in train set\n",
    "x = len(np.intersect1d(df_test.user_id.unique(), df_train.user_id.unique()))\n",
    "print(f'Users we can make predictions for in the test set: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users we can't make predictions for: 662\n"
     ]
    }
   ],
   "source": [
    "# Check number of users in the test set that we can't make predictions for\n",
    "y = len(df_test.user_id.unique()) - x\n",
    "print(f'Number of users we can\\'t make predictions for: {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article we can make predictions for in the test set: 574\n"
     ]
    }
   ],
   "source": [
    "#Check articles in test set which are also in train set which allow us to predict on them\n",
    "z = len(np.intersect1d(df_test.article_id.unique(), df_train.article_id.unique()))\n",
    "print(f'Article we can make predictions for in the test set: {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in the test set that we can't make predictions for: 0\n"
     ]
    }
   ],
   "source": [
    "# Check number of articles in the test set that we can't make predictions for\n",
    "a = len(df_test.article_id.unique()) - z\n",
    "print(f'Number of articles in the test set that we can\\'t make predictions for: {a}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awesome job!  That's right!  All of the test movies are in the training data, but there are only 20 test users that were also in the training set.  All of the other users that are in the test set we have no data on.  Therefore, we cannot make predictions for these users using SVD.\n"
     ]
    }
   ],
   "source": [
    "# Replace the values in the dictionary below\n",
    "a = 662 \n",
    "b = 574 \n",
    "c = 20 \n",
    "d = 0 \n",
    "\n",
    "\n",
    "sol_4_dict = {\n",
    "    'How many users can we make predictions for in the test set?': c, # letter here, \n",
    "    'How many users in the test set are we not able to make predictions for because of the cold start problem?':a, # letter here, \n",
    "    'How many movies can we make predictions for in the test set?':b, # letter here,\n",
    "    'How many movies in the test set are we not able to make predictions for because of the cold start problem?':d # letter here\n",
    "}\n",
    "\n",
    "t.sol_4_test(sol_4_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`5.` Now use the **user_item_train** dataset from above to find U, S, and V transpose using SVD. Then find the subset of rows in the **user_item_test** dataset that you can predict using this matrix decomposition with different numbers of latent features to see how many features makes sense to keep based on the accuracy on the test data. This will require combining what was done in questions `2` - `4`.\n",
    "\n",
    "Use the cells below to explore how well SVD works towards making predictions for recommendations on the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit SVD on the user_item_train matrix\n",
    "u_train, s_train, vt_train = np.linalg.svd(user_item_train) # fit svd similar to above then use the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4487, 4487)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these cells to see how well you can use the training \n",
    "# decomposition to predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_latent_feats = np.arange(10,700+10,20)\n",
    "sum_errs = []\n",
    "\n",
    "for k in num_latent_feats:\n",
    "    # restructure with k latent features\n",
    "    s_new, u_new, vt_new = np.diag(s_train[:k]), u_train[:, :k], vt_train[:k, :]\n",
    "    \n",
    "    # take dot product of the the matrix but only on user id's that we can predict for.\n",
    "    user_item_est = np.around(np.dot(np.dot(u_new, s_new), vt_new))[idx_test]\n",
    "    \n",
    "    # compute error for each prediction to actual value\n",
    "    diffs = np.subtract(user_item_matrix.iloc[idx_train], user_item_est)\n",
    "    \n",
    "    # total errors and keep track of them\n",
    "    err = np.sum(np.sum(np.abs(diffs)))\n",
    "    sum_errs.append(err)\n",
    "    \n",
    "    \n",
    "plt.plot(num_latent_feats, 1 - np.array(sum_errs)/df.shape[0]);\n",
    "plt.xlabel('Number of Latent Features');\n",
    "plt.ylabel('Accuracy');\n",
    "plt.title('Accuracy vs. Number of Latent Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`6.` Use the cell below to comment on the results you found in the previous question. Given the circumstances of your results, discuss what you might do to determine if the recommendations you make with any of the above recommendation systems are an improvement to how users currently find articles? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The results of the above are somewhat surprising. I would have expected the accuracy to increase as the number of latent factors increased, but this is not the case. Increasing the latent factors from 10 up to 700 actually resulted in a drop in accuracy of 0.5%.\n",
    "\n",
    "Perhaps utlising SVD to predict interactions in a binary yes or no is not the best application. Perhaps it will work better predicting values in a continuous range, such as a score between 0 - 10.\n",
    "\n",
    "Or perhaps the overlap between the train and test set is just too small. Having only 20 users on which to predict from an orignal base of over 5000 unique users is less than ideal.\n",
    "\n",
    "To evaluate the effectiveness of our recommendation engine, I would preform some A/B testing.\n",
    "\n",
    "Randomly split users, allocating half to the recommendation engine and half to the control case with no recommendations. We could measure the average number of articles being read by users in both groups. If our recommendation engine was suggesting relevant articles not seen by the users, you would hope that the average user article interactions would increase.\n",
    "\n",
    "You could also introduce a rating system on articles. In this case you would hope that recommended articles ranked higher on average than articles typically do overall. This information could then be fed back into our recommendation engine to improve the overall output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import call\n",
    "call(['python', '-m', 'nbconvert', 'Recommendations_with_IBM.ipynb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a class=\"anchor\" id=\"References\">Part VI: References</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1] LDA Analysis `LDA_IBM_Classification.ipynb`. For details on how the model was optimised for number of topics and removal of unnecessary stop words and more please see notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
